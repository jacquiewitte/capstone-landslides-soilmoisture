{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0ed63d0",
   "metadata": {},
   "source": [
    "# Earth Lab Capstone Project: Where can soil moisture improve rainfall-triggered landslide predictability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6ca49f",
   "metadata": {},
   "source": [
    "## Author: Jacquelyn Witte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250c1e46",
   "metadata": {},
   "source": [
    "## This Notebook examines the relationship between SMAP and ESA CCI soil moisture and GPM daily and IMERGE 30min precipitation for Landslides in the US\n",
    "\n",
    "- Based on Landslide events from the NASA Global Landslide Catalog\n",
    "- Using Landslide locations over Colorado as a workflow example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "829c65c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Override saving plots\n",
    "GLOBAL_CACHE_OVERRIDE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b8c4c7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import datetime as dt\n",
    "import earthpy as et\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import scipy.stats as st\n",
    "import seaborn as sns\n",
    "import xarray as xr\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Prettier plotting with seaborn\n",
    "sns.set(font_scale=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9784eed4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def read_smap(filepath, index):\n",
    "    \"\"\"\n",
    "    Reads SMAP data and returns the variable of interest.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath: Str\n",
    "        File path of a SMAP L3 HDF5 file\n",
    "        \n",
    "    group_id: String\n",
    "        Groups within the file to access\n",
    "        \n",
    "    index: int\n",
    "        Index associated with the variable to retrieve\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data: 2D numpy.ndarray (lat, lon)\n",
    "    date: Date String yyyymmdd\n",
    "    \"\"\"\n",
    "    # Read the file\n",
    "    group_id = 'Soil_Moisture_Retrieval_Data_PM'\n",
    "    with h5py.File(filepath, 'r') as f:\n",
    "        # Extract data info\n",
    "        data_id = list(f[group_id].keys())[index]\n",
    "        data = f[group_id][data_id][:,:]\n",
    "        data[data == f[group_id][data_id].attrs['_FillValue']] = np.nan\n",
    "        \n",
    "        filename = os.path.basename(filepath)\n",
    "        yyyymmdd= filename.split('_')[5]\n",
    "        yyyy = int(yyyymmdd[0:4])\n",
    "        mm = int(yyyymmdd[4:6])\n",
    "        dd = int(yyyymmdd[6:8])\n",
    "        date=dt.datetime(yyyy,mm,dd)\n",
    "    return data, date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd5b5ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def findfile(input_files, input_date):\n",
    "    \"\"\"\n",
    "    Returns a single file from a list of files\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_files: List of strings\n",
    "        List of full path to the file\n",
    "        \n",
    "    input_date: String\n",
    "        YYYYMMDD format\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    file: Str\n",
    "    \"\"\"\n",
    "    file = [x for x in input_files if re.findall(input_date, x)]\n",
    "    if not file:\n",
    "        raise ValueError('File does not exist for '+input_date)\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb117d2c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def nearestneighbor_ncdf(input_file, parameter, loc):\n",
    "    \"\"\"\n",
    "    Extracts nearest neighbor value based on location and desired parameter. \n",
    "    \n",
    "    Parameters\n",
    "    ----------   \n",
    "    input_file: Str - full path to a single file\n",
    "    \n",
    "    parameter: Str \n",
    "    \n",
    "    loc: tuple (degree longtitude, degree latitude)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "    \"\"\"\n",
    "    # read the netcdf file\n",
    "    try:\n",
    "        data_xr = xr.open_dataset(input_file).squeeze()\n",
    "    except IOError:\n",
    "        print(\"This file is not accessible: \"+input_file)\n",
    "    finally:\n",
    "        data_xr.close()\n",
    "    \n",
    "    # subset the file\n",
    "    res = data_xr[parameter].sel(indexers={\n",
    "            'lon': loc[0],\n",
    "            'lat': loc[1]},\n",
    "            method=\"nearest\")\n",
    "    \n",
    "    return float(res.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5a2a31",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def leastsq(x, y, method=3):\n",
    "    \"\"\"\n",
    "    5 methods to compute least squares fit.\n",
    "\n",
    "    Reference: https://github.com/sbird/spb_common/blob/master/leastsq.py\n",
    "    Compute the least squares fit to y = beta x + alpha,\n",
    "    using one of the 5 methods outlined in\n",
    "    http://adsabs.harvard.edu/abs/1990ApJ...364..104I\n",
    "    Method 1 minimises distance from Y given X (ie, the standard least squares fit)\n",
    "    Method 2 minimises distance from X given Y\n",
    "    Method 3 (recommended) is the OLS bisector, which gives a line bisecting the above two.\n",
    "    Method 4 (Orthogonal regression) minimises perpendicular distance from the line to points\n",
    "    Method 5 is the geometric mean of the slopes from methods 1 and 2.\n",
    "    Method 6 is the Theil-Sen estimator: the median of the pairwise slopes.\n",
    "    (See Akritas 95,  http://www.tandfonline.com/doi/abs/10.1080/01621459.1995.10476499)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y: numeric array\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (alpha, beta, bvar), the intercept slope and variance of the slope\n",
    "    \"\"\"\n",
    "    # Define some sums\n",
    "    xbar = np.mean(x)\n",
    "    ybar = np.mean(y)\n",
    "    xdif = x-xbar\n",
    "    ydif = y-ybar\n",
    "    sxx = np.sum(xdif**2)\n",
    "    syy = np.sum(ydif**2)\n",
    "    sxy = np.sum(ydif*xdif)\n",
    "\n",
    "    # Check for zeros\n",
    "    if sxx == 0 or syy == 0 or sxy == 0:\n",
    "        raise ValueError(\"Least Squares ill-defined\")\n",
    "    if method > 6 or method < 1:\n",
    "        raise ValueError(\"Method not recognised\")\n",
    "\n",
    "    # These formulas are taken from Table 1 of Isobe et al, page 3\n",
    "    # Minimise distance from Y given X\n",
    "    beta1 = sxy/sxx\n",
    "    # Variance of b1\n",
    "    bvar1 = np.sum(xdif**2*(ydif-beta1*xdif)**2)/sxx**2\n",
    "    # Minimise distance from X given Y\n",
    "    beta2 = syy/sxy\n",
    "    # Variance of b2\n",
    "    bvar2 = np.sum(ydif**2*(ydif-beta2*xdif)**2)/sxy**2\n",
    "    # Covariance of b1 and b2\n",
    "    covb12 = np.sum(xdif*ydif*(ydif-beta2*xdif) *\n",
    "                    (ydif-beta1*xdif))/(beta1*sxx**2)\n",
    "\n",
    "    if method == 1:\n",
    "        beta = beta1\n",
    "        bvar = bvar1\n",
    "    if method == 2:\n",
    "        beta = beta2\n",
    "        bvar = bvar2\n",
    "    if method == 3:\n",
    "        # OLS bisector: line that bisects the above two.\n",
    "        beta1p1 = 1+beta1**2\n",
    "        beta2p1 = 1+beta2**2\n",
    "        beta = (beta1*beta2 - 1 + np.sqrt(beta1p1*beta2p1))/(beta1+beta2)\n",
    "        # Variance\n",
    "        prefac = beta**2 / ((beta1 + beta2)**2 * beta1p1 * beta2p1)\n",
    "        var = (beta2p1**2 * bvar1 + 2 * beta1p1 * beta2p1 \n",
    "               * covb12 + beta1p1**2 * bvar2)\n",
    "        bvar = prefac*var\n",
    "\n",
    "    if method == 4:\n",
    "        # Orthogonal: minimise perpendicular distance from line to points\n",
    "        beta = 0.5*((beta2-1./beta1)+np.sign(sxy) *\n",
    "                    np.sqrt(4+(beta2-1./beta1)**2))\n",
    "        prefac = beta**2 / (4*beta1**2 + (beta1*beta2 - 1)**2)\n",
    "        bvar = prefac * (bvar1/beta1**2 + 2*covb12 + beta1**2*bvar2)\n",
    "\n",
    "    if method == 5:\n",
    "        # Reduced major axis:\n",
    "        beta = np.sign(sxy)*np.sqrt(beta1*beta2)\n",
    "        bvar = 0.25 * (beta2/beta1 * bvar1 + 2*covb12 + beta1/beta2 * bvar2)\n",
    "\n",
    "    if method == 6:\n",
    "        # Theil-Sen estimator for uncensored data: the median of the slopes.\n",
    "        yy = np.subtract.outer(y, y)\n",
    "        xx = np.subtract.outer(x, x)\n",
    "        ind = np.where(xx != 0)\n",
    "        beta = np.median(yy[ind]/xx[ind])\n",
    "        # Can't find a formula for the variance\n",
    "        bvar = 0\n",
    "\n",
    "    # The intercept\n",
    "    alpha = ybar - beta*xbar\n",
    "\n",
    "    return (alpha, beta, bvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a1d739",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_imerge_hires(imerge_files, glc_df):\n",
    "    \"\"\"\n",
    "    Reads all IMERGE 30min CSV file into a dataFrame\n",
    "\n",
    "    \n",
    "    # Ref: https://www.geeksforgeeks.org/ways-to-filter-pandas-dataframe-by-column-values/\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    imerge_files: List of strings\n",
    "        List of full path to the file\n",
    "    \n",
    "    glc_df: dataFrame\n",
    "        Global Landslide Catalog \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    imerge: dataFrame\n",
    "        Contains datetime, landslide ID, precipitation\n",
    "\n",
    "    \"\"\"\n",
    "    id_list = glc_df['event_id'].values.tolist()\n",
    "    \n",
    "    list = []\n",
    "    for f in imerge_files:\n",
    "        #temp_df = pd.read_csv(f).set_index('datetime')\n",
    "        print(f)\n",
    "        temp_df = pd.read_csv(f)\n",
    "        # filter for landslide id\n",
    "        list.append(temp_df[temp_df['id'].isin(id_list)])\n",
    "\n",
    "    imerge = pd.concat(list)\n",
    "    # convert datetime to pd datetime because some dates are not in the right format\n",
    "    imerge['datetime'] = pd.to_datetime(imerge['datetime'])\n",
    "    # Create a simple date string to compare with the GLC data\n",
    "    imerge['yyyymmdd'] = pd.to_datetime(imerge['datetime']).dt.strftime('%Y%m%d')\n",
    "    #imerge.index = pd.to_datetime(imerge.index)\n",
    "    imerge = imerge.reset_index().set_index('datetime')\n",
    "    return imerge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9bdb63",
   "metadata": {},
   "source": [
    "### Choose the state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850deda2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "westernUS = ['Colorado', 'California', 'Oregon', \n",
    "             'Washington','Utah', 'Idaho']\n",
    "state = westernUS[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd2a957",
   "metadata": {},
   "source": [
    "### Read and subset to Landslides >= year 2015 (SMAP data starts in 2015)\n",
    "- Based on the state chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbb95b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read the GLC file\n",
    "glc_file = os.path.join(et.io.HOME,\n",
    "                        'earth-analytics',\n",
    "                        'data', 'capstone', \n",
    "                        'landslide', 'nasa_global_landslide_catalog_point.csv')\n",
    "\n",
    "glc = pd.read_csv(glc_file)\n",
    "\n",
    "glc_state = glc[(glc['country_code'] == 'US') \n",
    "             & (glc['admin_division_name'] == state) \n",
    "             & (glc['landslide_trigger'] != 'freeze_thaw') \n",
    "             & (glc['landslide_trigger'] != 'snowfall_snowmelt') \n",
    "             & (glc['landslide_trigger'] != 'earthquake') \n",
    "             & (glc['landslide_trigger'] != 'leaking_pipe') \n",
    "             & (glc['landslide_trigger'] != 'no_apparent_trigger') \n",
    "             & (glc['landslide_trigger'] != 'other')              \n",
    "             & (glc['landslide_trigger'] != 'unknown')\n",
    "            ]\n",
    "# convert to pandas datetime\n",
    "glc_state['date'] = pd.to_datetime(glc_state['event_date'])\n",
    "glc_state = glc_state.set_index('date').sort_index()\n",
    "glc_state_gt2015 = glc_state[glc_state.index > '2015-04-01']\n",
    "\n",
    "print(glc_state_gt2015.shape)\n",
    "print(np.unique(glc_state_gt2015['landslide_trigger']))\n",
    "print(np.unique(glc_state_gt2015['landslide_category']))\n",
    "print(glc_state_gt2015.columns)\n",
    "\n",
    "# Dropping last dataframe if state=Utah - SMAP data does not exist\n",
    "if state == 'Utah':\n",
    "    glc_state_gt2015 = glc_state_gt2015.drop(pd.to_datetime('2019-06-26 04:00:00'))\n",
    "\n",
    "glc_state_gt2015.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd883ca8",
   "metadata": {},
   "source": [
    "### Get all SMAP, ESA, GPM and IMERGE data files, sorted\n",
    " - ESA = Percent of Saturation Soil Moisture\n",
    " - SMAP = Volumetric soil moisture in cm3/cm3\n",
    " - GPM = Daily precipitation in mm\n",
    " - IMERGE = 30 min precipitation in mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879b569d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_dir = os.path.join(et.io.HOME,\n",
    "                        'earth-analytics',\n",
    "                        'data', 'capstone')\n",
    "smap_files = sorted(glob(os.path.join(data_dir, 'smap_9km', '*.h5')))\n",
    "\n",
    "# GPM daily files\n",
    "gpm_files = sorted(glob(os.path.join(data_dir, 'gpm_westernUS', '*nc4')))\n",
    "\n",
    "# GPM 30 min files\n",
    "gpm_hires_files = sorted(glob(os.path.join(data_dir,\n",
    "                                           'precip_imerge',\n",
    "                                           'imerge',\n",
    "                                           'glc', 'imerge*.csv')))\n",
    "\n",
    "esa_files = sorted(glob(os.path.join(data_dir, 'esa_soil_moisture',\n",
    "                                     '*ACTIVE*nc')))\n",
    "\n",
    "# Print a sample as a sanity check\n",
    "print(os.path.exists(smap_files[0]))\n",
    "print(os.path.exists(gpm_files[0]))\n",
    "print(os.path.exists(gpm_hires_files[0]))\n",
    "print(os.path.exists(esa_files[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b7d1a1",
   "metadata": {},
   "source": [
    "### Load the EASE2 grid lon and lat datasets to subset SMAP data. \n",
    "- These can be found on the NSIDC website: https://nsidc.org/data/ease/tools#geo_data_files\n",
    "\n",
    "> Brodzik, M. J., B. Billingsley, T. Haran, B. Raup, M. H. Savoie. 2012. EASE-Grid 2.0: Incremental but Significant Improvements for Earth-Gridded Data Sets. ISPRS International Journal of Geo-Information, 1(1):32-45, doi:10.3390/ijgi1010032. http://www.mdpi.com/2220-9964/1/1/32."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d30c6e1",
   "metadata": {},
   "source": [
    "#### These are SMAP variables that can provide key information in characterizing landslides over Colorado\n",
    "\n",
    "| Variable Name | Index | Units |\n",
    "| ----------- | ----------- | ----------- |\n",
    "| soil_moisture | 24 | cm<sup>3</sup>/cm<sup>3</sup> |\n",
    "| radar_water_body_fraction | 15 | N/A | \n",
    "| vegetation_opacity | 46 | N/A |\n",
    "| vegetation_water_content | 50 | kg/m<sup>2</sup> |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c959ee75",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reshape to dimensions of the SMAP data above\n",
    "sm_data, date = read_smap(smap_files[1], 24)\n",
    "\n",
    "lats = np.fromfile(os.path.join(data_dir, 'smap_9km',\n",
    "                                'EASE2_M09km.lats.3856x1624x1.double'),\n",
    "                   dtype=np.float64).reshape(sm_data.shape)\n",
    "lons = np.fromfile(os.path.join(data_dir, 'smap_9km',\n",
    "                                'EASE2_M09km.lons.3856x1624x1.double'),\n",
    "                   dtype=np.float64).reshape(sm_data.shape)\n",
    "sm_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f072f67",
   "metadata": {},
   "source": [
    "### Read the SMAP, ESA CCI and GPM data \n",
    "- Two dataframes are generated\n",
    "    1. Colocated to all the Landslide events\n",
    "    2. Precipitation measurements going back 7 day from each landslide event and indexed to Landslide ID\n",
    "        - GPM daily resolution precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b573d356",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "precip7d_date = []\n",
    "precip7d = []\n",
    "precip_accum = []\n",
    "precip_max = []\n",
    "smap_sm = []\n",
    "smap_sm_7d = []\n",
    "smap_wc = []\n",
    "esa_sm = []\n",
    "esa_sm_7d = []\n",
    "landslide_date = []\n",
    "landslide_id = []\n",
    "landslide7d_id = []\n",
    "landslide_cat = []\n",
    "landslide_trig = []\n",
    "landslide_sz = []\n",
    "lat = []\n",
    "lon = []\n",
    "periods = 7\n",
    "count_down = []\n",
    "\n",
    "for i, ls_date in enumerate(glc_state_gt2015.index):\n",
    "\n",
    "    # Get -7 days from the event\n",
    "    glc_date = pd.date_range(ls_date, periods=periods,\n",
    "                             freq='-1D').strftime('%Y%m%d')\n",
    "\n",
    "    # Append landslide metadata\n",
    "    landslide_date.append(ls_date)\n",
    "    lat.append(glc_state_gt2015.latitude[i])\n",
    "    lon.append(glc_state_gt2015.longitude[i])\n",
    "    landslide_id.append(glc_state_gt2015.event_id[i])\n",
    "    landslide_cat.append(glc_state_gt2015.landslide_category[i])\n",
    "    landslide_trig.append(glc_state_gt2015.landslide_trigger[i])\n",
    "    landslide_sz.append(glc_state_gt2015.landslide_size[i])\n",
    "\n",
    "    # Take the +/- 0.3 deg mean around the Landslide event\n",
    "    N_lat = glc_state_gt2015.latitude[i]+0.15\n",
    "    S_lat = glc_state_gt2015.latitude[i]-0.15\n",
    "    W_lon = glc_state_gt2015.longitude[i]-0.15\n",
    "    E_lon = glc_state_gt2015.longitude[i]+0.15\n",
    "    subset = (lats < N_lat) & (lats > S_lat) & (lons > W_lon) & (lons < E_lon)\n",
    "\n",
    "    #print(i, ls_date, glc_state_gt2015.latitude[i], glc_state_gt2015.longitude[i])\n",
    "    \n",
    "    # Initialize data\n",
    "    sm_max = []\n",
    "    vegwc_max = []\n",
    "    esa_mean = []\n",
    "    precip = []\n",
    "    countd = periods\n",
    " \n",
    "    # loop over the 7 days\n",
    "    for yyyymmdd in glc_date:\n",
    "        \n",
    "        # Find the SMAP file\n",
    "        filesm = findfile(smap_files, yyyymmdd)\n",
    "        # Retrieve the SMAP variables\n",
    "        sm, time_t = read_smap(filesm[0], 24)\n",
    "        vegwc, t = read_smap(filesm[0], 50)\n",
    "        # Calculate the SMAP max\n",
    "        sm_max.append(np.nanmax(sm[subset]))\n",
    "        vegwc_max.append(np.nanmax(vegwc[subset]))\n",
    "        \n",
    "        # Get the Landslide location\n",
    "        loc = (glc_state_gt2015.longitude[i], glc_state_gt2015.latitude[i])\n",
    "\n",
    "        # Find the ESA soil moisture file\n",
    "        file_esa = findfile(esa_files, yyyymmdd) \n",
    "        # Get the nearest neighbor value of % soil moisture\n",
    "        res_esa = nearestneighbor_ncdf(file_esa[0], 'sm', loc)\n",
    "        # Replace negative values with NaN\n",
    "        if res_esa < 0.0:\n",
    "            esa_mean.append(np.nan)\n",
    "        else:\n",
    "            esa_mean.append(res_esa)\n",
    "        \n",
    "        # find the GPM file\n",
    "        file_gpm = findfile(gpm_files, yyyymmdd) \n",
    "        \n",
    "        precip7d.append(nearestneighbor_ncdf(file_gpm[0], \n",
    "                             'precipitationCal', loc))\n",
    "        landslide7d_id.append(glc_state_gt2015.event_id[i])\n",
    "        # Append the date\n",
    "        precip7d_date.append(yyyymmdd)\n",
    "\n",
    "        precip.append(nearestneighbor_ncdf(file_gpm[0], \n",
    "                             'precipitationCal', loc))\n",
    "        \n",
    "        # Append countdown\n",
    "        count_down.append(countd)\n",
    "        countd -= 1\n",
    "\n",
    "    # Append the summary values for the 7 day period\n",
    "    smap_sm.append(np.nanmax(sm_max))\n",
    "    smap_wc.append(np.nanmax(vegwc_max))\n",
    "    esa_sm.append(np.nanmax(esa_mean))\n",
    "    smap_sm_7d.extend(sm_max)\n",
    "    esa_sm_7d.extend(esa_mean)\n",
    "    \n",
    "    # Filter for low precipitation values\n",
    "    if np.nansum(precip) < 0.4:\n",
    "        precip_accum.append(np.nan)\n",
    "        precip_max.append(np.nan) \n",
    "    else:\n",
    "        precip_accum.append(np.nansum(precip))\n",
    "        precip_max.append(np.nanmax(precip))\n",
    "    \n",
    "# Create a soils and precip dataFrame\n",
    "landslide_df = pd.DataFrame(smap_sm,\n",
    "                            index=pd.to_datetime(landslide_date), \n",
    "                            columns=['smap_sm'])\n",
    "landslide_df['veg_water_content'] = smap_wc\n",
    "landslide_df['esa_sm_percent'] = esa_sm\n",
    "landslide_df['gpm_7day_accum_mm'] = precip_accum\n",
    "landslide_df['gpm_7day_max_mm'] = precip_max\n",
    "\n",
    "# Add the Landslide metadata\n",
    "landslide_df['glc_lat'] = lat\n",
    "landslide_df['glc_lon'] = lon\n",
    "landslide_df['landslide_id'] = landslide_id\n",
    "landslide_df['landslide_category'] = landslide_cat\n",
    "landslide_df['landslide_trigger'] = landslide_trig\n",
    "landslide_df['landslide_size'] = landslide_sz\n",
    "\n",
    "# Create the 7day precipitation dataFrame\n",
    "landslide_precip7d_df = pd.DataFrame(precip7d,\n",
    "                            index=pd.to_datetime(precip7d_date), \n",
    "                            columns=['gpm_precip_mm'])\n",
    "landslide_precip7d_df['landslide_id'] = landslide7d_id\n",
    "landslide_precip7d_df['smap_sm']= smap_sm_7d\n",
    "landslide_precip7d_df['esa_vol']= esa_sm_7d\n",
    "\n",
    "\n",
    "# Add the cumulative precipitation\n",
    "accum = []\n",
    "for i, data in landslide_precip7d_df.groupby(\"landslide_id\"):\n",
    "    res = data['gpm_precip_mm'].cumsum().values\n",
    "    if res.max() > 0.4:\n",
    "        accum.extend(np.flip(data['gpm_precip_mm'].cumsum().values))\n",
    "    else:\n",
    "        accum.extend([np.nan]*periods)\n",
    "\n",
    "landslide_precip7d_df['gpm_7day_accum_mm'] = accum\n",
    "landslide_precip7d_df['days-to-landslide'] = count_down\n",
    "\n",
    "# Add the normalized precipitation\n",
    "accum_norm = []\n",
    "for i, data in landslide_precip7d_df.groupby(\"landslide_id\"):\n",
    "    res = data['gpm_7day_accum_mm']/data['gpm_7day_accum_mm'].mean()\n",
    "    accum_norm.append(res.max())\n",
    "\n",
    "landslide_df['gpm_7day_accum_norm'] = accum_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bd732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "landslide_precip7d_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32312b1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "landslide_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f032486",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Add the IMERGE 30min resolution precipitation to the landslide_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd91886",
   "metadata": {},
   "source": [
    "#### First, read the IMERGE 30min data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4078f4d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "imerge_30min = get_imerge_hires(gpm_hires_files, glc_state_gt2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bc806f",
   "metadata": {},
   "source": [
    "#### Loop over the newly created landslide_df\n",
    "- Add the sum and the max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc7752a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "imerge_7day_sum = []\n",
    "imerge_7day_max = []\n",
    "imerge_7day_accum = []\n",
    "imerge_7day_date = []\n",
    "imerge_7day_id = []\n",
    "\n",
    "for i, ls_date in enumerate(landslide_df.index):\n",
    "\n",
    "    glc_date = pd.date_range(ls_date, periods=7,\n",
    "                             freq='-1D').strftime('%Y%m%d').to_list()\n",
    "\n",
    "    # selecting rows based on condition\n",
    "    rslt_df = imerge_30min[(imerge_30min['id'] == landslide_df['landslide_id'][i]) &\n",
    "                           imerge_30min['yyyymmdd'].isin(glc_date)]\n",
    "    # first calculate the 7 day precipitation stats\n",
    "    if rslt_df.shape[0] > 0:\n",
    "        imerge_7day_sum.append(rslt_df['precipitation'].sum())\n",
    "        imerge_7day_max.append(rslt_df['precipitation'].max())\n",
    "        # append the 7 day daily values\n",
    "        imerge_7day_accum.extend(rslt_df.precipitation.resample('D').sum())\n",
    "        imerge_7day_date.extend(glc_date)\n",
    "        imerge_7day_id.extend([landslide_df['landslide_id'][i]]*7)\n",
    "    else:\n",
    "        imerge_7day_sum.append(np.nan)\n",
    "        imerge_7day_max.append(np.nan)\n",
    "        imerge_7day_accum.extend([np.nan]*7)\n",
    "        imerge_7day_date.extend(glc_date)\n",
    "        imerge_7day_id.extend([landslide_df['landslide_id'][i]]*7)\n",
    "\n",
    "landslide_df['imerge_7day_accum_mm'] = imerge_7day_sum\n",
    "landslide_df['imerge_7day_max_mm'] = imerge_7day_max\n",
    "landslide_df.index.name = 'date'\n",
    "\n",
    "landslide_precip7d_df['imerge_precip_mm'] = imerge_7day_accum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c6602e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "landslide_precip7d_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c617324",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# calculate the number of landslides per year\n",
    "landslide_count = landslide_precip7d_df.groupby(landslide_precip7d_df.index.year)['smap_sm'].nunique().values.tolist()\n",
    "print(landslide_count)\n",
    "# year = np.arange(len(landslide_count))+2015\n",
    "year = []\n",
    "for i, df in landslide_df.groupby(landslide_df.index.year):\n",
    "    year.append(df.index.year[0])\n",
    "\n",
    "colors = ['dodgerblue',  'orange', 'firebrick', 'forestgreen', 'k', 'purple']\n",
    "year_count = []\n",
    "for i, yr in enumerate(year):\n",
    "    year_count.append(str(yr)+' ('+str(landslide_count[i])+')')\n",
    "    \n",
    "landslide_dict = dict(zip(year_count, colors))\n",
    "color_dict = dict(zip(year, colors))\n",
    "\n",
    "print(landslide_dict)\n",
    "print(color_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5244a1",
   "metadata": {},
   "source": [
    "### Plot the cumulative precipitation going back 7 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8702ff33",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 5), sharex=True, sharey=True)\n",
    "\n",
    "for id, df in landslide_precip7d_df.groupby('landslide_id'):\n",
    "    result = df['gpm_7day_accum_mm']/df['gpm_7day_accum_mm'].mean()\n",
    "    if result.max() > 1.1:\n",
    "        ax.plot(df['days-to-landslide'], result,\n",
    "                color=color_dict[result.index.year[0]])\n",
    "\n",
    "# Add legend\n",
    "leg = ax.legend(handlelength=0, handletextpad=0, fancybox=True,\n",
    "                labels=landslide_dict, labelcolor=color_dict.values(),\n",
    "                facecolor='white')\n",
    "for item in leg.legendHandles:\n",
    "    item.set_visible(False)\n",
    "\n",
    "# ax.invert_xaxis()\n",
    "ax.axvline(x=7, ymin=0, color='mediumorchid', linestyle='dashed', linewidth=5)\n",
    "\n",
    "ax.set(xlabel='Days leading up to rainfall-landslide event',\n",
    "       ylabel='Normalized Precipitation',\n",
    "       title=('GPM Cumultive Daily Rainfall over '\n",
    "              + state+'\\nNormalized to the 7-day mean'))\n",
    "ax.text(7.1, 0, 'Landslide\\nTriggered',\n",
    "        color='mediumorchid', fontsize='medium')\n",
    "\n",
    "# output the figure\n",
    "output_fig = os.path.join(et.io.HOME,\n",
    "                             'earth-analytics',\n",
    "                             'capstone-landslides-soilmoisture', 'plots',\n",
    "                             'gpm_precip7d_normalized_timeseries_'+state+'.png')\n",
    "\n",
    "cache_override = False or GLOBAL_CACHE_OVERRIDE\n",
    "if not os.path.exists(output_fig) or cache_override:\n",
    "    plt.savefig(output_fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457b3d08",
   "metadata": {},
   "source": [
    "### Plot Precipitation vs SMAP Soil Moisture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5b7fd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "category = dict(zip(np.unique(landslide_df['landslide_category']), colors))\n",
    "print(category)\n",
    "\n",
    "trigger = dict(zip(np.unique(landslide_df['landslide_trigger']), colors))\n",
    "print(trigger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907ff0c2",
   "metadata": {},
   "source": [
    "#### Plot by Landslide Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8906d63",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a scatter plot\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(ncols=2, nrows=2, \n",
    "                                             figsize=(12, 12))\n",
    "\n",
    "# GPM daily precip vs SMAP & ESA\n",
    "for id, df in landslide_df.groupby(\"landslide_id\"):\n",
    "    for cat in category:\n",
    "        if df['landslide_category'][0] == cat:\n",
    "            ax1.scatter(df['smap_sm'], df['gpm_7day_accum_mm'],\n",
    "                        edgecolor='k', s=50.0,\n",
    "                        color=category[cat])\n",
    "            ax2.scatter(df['esa_sm_percent'], df['gpm_7day_accum_mm'],\n",
    "                        edgecolor='k', s=50.0,\n",
    "                        color=category[cat])\n",
    "\n",
    "# calculate the linear trend\n",
    "idx = ( np.isfinite(landslide_df['smap_sm']) \n",
    "        & np.isfinite(landslide_df['gpm_7day_accum_mm'])\n",
    "       )\n",
    "# defining the variables\n",
    "x = landslide_df['smap_sm'][idx].tolist()\n",
    "y = landslide_df['gpm_7day_accum_mm'][idx].tolist()\n",
    "# Calculate the OLR Bisector fit\n",
    "intercept, slope, var = leastsq(x, y, method=3)\n",
    "trend_mean_text = 'slope: '+str('{:.2f}'.format(slope)) + \\\n",
    "      ', y: '+str('{:.2f}'.format(intercept))\n",
    "\n",
    "# plot the linear trend\n",
    "ax1.plot(landslide_df['smap_sm'],\n",
    "         intercept + slope*landslide_df['smap_sm'],\n",
    "         color='crimson',\n",
    "         linewidth=1.0)\n",
    "ax1.text(0.15, 110, trend_mean_text)\n",
    "\n",
    "ax1.set(xlabel='SMAP Soil moisture [$cm^3 cm^{-3}$]',\n",
    "        ylabel='GPM 7day Precipitation Accummulation [mm]')\n",
    "#ax1.set_ylim([-5, 120])\n",
    "\n",
    "ax2.set(xlabel='ESA CCI Soil Moisture Saturation [%]',\n",
    "        ylabel='GPM 7day Precipitation Accummulation [mm]')\n",
    "#ax2.set_ylim([-5, 100])\n",
    "\n",
    "# Add legend\n",
    "leg = ax1.legend(handlelength=0, handletextpad=0, fancybox=True,\n",
    "                 labels=category, labelcolor=colors, facecolor='white')\n",
    "for item in leg.legendHandles:\n",
    "    item.set_visible(False)\n",
    "\n",
    "# add an overall caption\n",
    "fig.suptitle(\n",
    "    'Precipitaton vs Soil Moisture for 2015-2020 '+state+' Landslides'\n",
    ")\n",
    "\n",
    "# IMERGE 30min precip vs SMAP & ESA\n",
    "for id, df in landslide_df.groupby(\"landslide_id\"):\n",
    "    for cat in category:\n",
    "        if df['landslide_category'][0] == cat:\n",
    "            ax3.scatter(df['smap_sm'], df['imerge_7day_accum_mm'],\n",
    "                        edgecolor='k', s=50.0,\n",
    "                        color=category[cat])\n",
    "            ax4.scatter(df['esa_sm_percent'], df['imerge_7day_accum_mm'],\n",
    "                        edgecolor='k', s=50.0,\n",
    "                        color=category[cat])\n",
    "ax3.set(xlabel='SMAP Soil moisture [$cm^3 cm^{-3}$]',\n",
    "        ylabel='IMERGE 7day Precipitation Accummulation [mm]')\n",
    "#ax3.set_ylim([-5, 120])\n",
    "\n",
    "ax4.set(xlabel='ESA CCI Soil Moisture Saturation [%]',\n",
    "        ylabel='IMERGE 7day Precipitation Accummulation [mm]')\n",
    "\n",
    "# calculate the linear trend\n",
    "idx = ( np.isfinite(landslide_df['smap_sm']) \n",
    "        & np.isfinite(landslide_df['imerge_7day_accum_mm'])\n",
    "       )\n",
    "# defining the variables\n",
    "x = landslide_df['smap_sm'][idx].tolist()\n",
    "y = landslide_df['imerge_7day_accum_mm'][idx].tolist()\n",
    "# Calculate the OLR Bisector fit\n",
    "intercept, slope, var = leastsq(x, y, method=3)\n",
    "trend_mean_text = 'slope: '+str('{:.2f}'.format(slope)) + \\\n",
    "      ', y: '+str('{:.2f}'.format(intercept))\n",
    "\n",
    "# plot the linear trend\n",
    "ax3.plot(landslide_df['smap_sm'],\n",
    "         intercept + slope*landslide_df['smap_sm'],\n",
    "         color='crimson',\n",
    "         linewidth=1.0)\n",
    "ax3.text(0.25, 110, trend_mean_text)\n",
    "\n",
    "\n",
    "\n",
    "# Add legend\n",
    "leg = ax3.legend(handlelength=0, handletextpad=0, fancybox=True,\n",
    "                 labels=category, labelcolor=colors, facecolor='white')\n",
    "for item in leg.legendHandles:\n",
    "    item.set_visible(False)\n",
    "\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "# output the figure\n",
    "output_fig = os.path.join(et.io.HOME,\n",
    "                          'earth-analytics',\n",
    "                          'capstone-landslides-soilmoisture', 'plots',\n",
    "                          'sm_VS_precip_2015_2020_category_'+state+'.png')\n",
    "\n",
    "cache_override = False or GLOBAL_CACHE_OVERRIDE\n",
    "if not os.path.exists(output_fig) or cache_override:\n",
    "    plt.savefig(output_fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5c837d",
   "metadata": {},
   "source": [
    "#### Plot by Landslide Trigger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6e7f78",
   "metadata": {},
   "source": [
    "### Scatter plots - Total precipitation vs maximum precipitation intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe9d1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "landslide_precip7d_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53728c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "# GPM daily Pre3cip vs SMAP & ESA\n",
    "xdata = []\n",
    "ydata = []\n",
    "for id, df in landslide_precip7d_df.groupby(\"landslide_id\"):\n",
    "    if df['gpm_precip_mm'].max() < 100.:\n",
    "        ax.scatter((df['smap_sm'].max()-df['smap_sm'].min()), df['gpm_precip_mm'].max(), \n",
    "                        color='dodgerblue', edgecolor='k', s=60.0)\n",
    "        xdata.append(df['smap_sm'].max()-df['smap_sm'].min())\n",
    "        ydata.append(df['gpm_precip_mm'].max())\n",
    "    \n",
    "\n",
    "# Calculate the OLR Bisector fit\n",
    "intercept, slope, var = leastsq(xdata, ydata, method=5)\n",
    "\n",
    "trend_mean_text = 'slope: '+str('{:.2f}'.format(slope)) + \\\n",
    "      ', y: '+str('{:.2f}'.format(intercept))\n",
    "\n",
    "# plot the linear trend\n",
    "ax.plot(np.array(xdata),\n",
    "         intercept + slope*np.array(xdata),\n",
    "         color='crimson',\n",
    "         linewidth=2.0)\n",
    "ax.text(0.02, 50, trend_mean_text);\n",
    "\n",
    "# Add titles\n",
    "ax.set(xlabel='SMAP Soil Moisture 7day Difference [$cm^3 cm^{-3}$]',\n",
    "        ylabel='GPM Precipitation Intensity [mm]');\n",
    "\n",
    "# add an overall caption\n",
    "fig.suptitle(\n",
    "    'Precipitaton vs Soil Moisture\\n2015-2020 '+state+' Landslides'\n",
    ");\n",
    "\n",
    "#output the figure\n",
    "output_fig = os.path.join(et.io.HOME,\n",
    "            'earth-analytics',\n",
    "            'capstone-landslides-soilmoisture', 'plots',\n",
    "            'smdiff_VS_precipintensity_2015_2020_'+state+'.png')\n",
    "\n",
    "cache_override = False or GLOBAL_CACHE_OVERRIDE\n",
    "if not os.path.exists(output_fig) or cache_override:\n",
    "    plt.savefig(output_fig);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8baaf0",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Look at outliers by creating box-whisker plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37500161",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "\n",
    "red_sqr = dict(markerfacecolor='r', marker='s')\n",
    "\n",
    "# Data may contain NaN - these have to be removed for boxplot() to work\n",
    "filtered_data = landslide_df['smap_sm'][~np.isnan(landslide_df['smap_sm'])]\n",
    "\n",
    "axs[0].boxplot(filtered_data, flierprops=red_sqr,\n",
    "              whiskerprops = dict(linestyle='-',linewidth=2., color='steelblue'),\n",
    "              boxprops = dict(linestyle='-',linewidth=2., color='steelblue'),\n",
    "              medianprops = dict(linestyle='-',linewidth=3., color='orange'),\n",
    "              capprops = dict(linestyle='-',linewidth=2., color='steelblue')\n",
    "              )\n",
    "axs[0].set(title='SMAP',\n",
    "          ylabel='$cm^3 cm^{-3}$')\n",
    "\n",
    "filtered_data = landslide_df['gpm_7day_accum_mm'][~np.isnan(landslide_df['gpm_7day_accum_mm'])]\n",
    "axs[1].boxplot(filtered_data, flierprops=red_sqr,\n",
    "              whiskerprops = dict(linestyle='-',linewidth=2., color='steelblue'),\n",
    "              boxprops = dict(linestyle='-',linewidth=2., color='steelblue'),\n",
    "              medianprops = dict(linestyle='-',linewidth=3., color='orange'),\n",
    "              capprops = dict(linestyle='-',linewidth=2., color='steelblue')\n",
    "              )\n",
    "axs[1].set(title='GPM',\n",
    "          ylabel='Accumulated Precip [mm]')\n",
    "\n",
    "plt.suptitle(state+' 2015-2020 Landslide Events')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "output_fig = os.path.join(et.io.HOME,\n",
    "            'earth-analytics',\n",
    "            'capstone-landslides-soilmoisture', 'plots',\n",
    "            'sm_precip_2015_2020_boxplot_'+state+'.png')\n",
    "\n",
    "#output the figure\n",
    "cache_override = False or GLOBAL_CACHE_OVERRIDE\n",
    "if not os.path.exists(output_fig) or cache_override:\n",
    "    plt.savefig(output_fig);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd91e0c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fly = box['fliers'][0]\n",
    "# outliers = fly.get_ydata()\n",
    "# outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce25a87a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
